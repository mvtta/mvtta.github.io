<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.555">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Mariana Emauz Valdetaro">
<meta name="dcterms.date" content="2024-10-10">

<title>/ - Synthetic Intelligence &amp; Moving Bodies</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="../../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">


<link rel="stylesheet" href="../../styles.css">
<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">/</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../posts/agency/index.html"> 
<span class="menu-text">Agency</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts/essays/index.html"> 
<span class="menu-text">Essays</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts/experiments/index.html"> 
<span class="menu-text">Experiments</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts/notebooks/index.html"> 
<span class="menu-text">Notebooks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts/thoughts/index.html"> 
<span class="menu-text">Thoughts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../resource-tree.html"> 
<span class="menu-text">Resource-Tree</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about-me.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#synthetic-intelligence-moving-bodies" id="toc-synthetic-intelligence-moving-bodies" class="nav-link active" data-scroll-target="#synthetic-intelligence-moving-bodies">Synthetic Intelligence &amp; Moving Bodies</a>
  <ul>
  <li><a href="#abstract" id="toc-abstract" class="nav-link" data-scroll-target="#abstract">Abstract</a></li>
  <li><a href="#notes" id="toc-notes" class="nav-link" data-scroll-target="#notes">Notes</a></li>
  </ul></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a>
  <ul>
  <li><a href="#working-with-intelligence" id="toc-working-with-intelligence" class="nav-link" data-scroll-target="#working-with-intelligence"><span class="header-section-number">1.0.1</span> Working with <em>Intelligence</em>?</a></li>
  <li><a href="#defining-ai-and-ml-a-spectrum-of-intelligence" id="toc-defining-ai-and-ml-a-spectrum-of-intelligence" class="nav-link" data-scroll-target="#defining-ai-and-ml-a-spectrum-of-intelligence"><span class="header-section-number">1.0.2</span> Defining AI and ML: A Spectrum of Intelligence</a></li>
  </ul></li>
  <li><a href="#ebbs-flows-information" id="toc-ebbs-flows-information" class="nav-link" data-scroll-target="#ebbs-flows-information"><span class="header-section-number">2</span> Ebbs, Flows &amp; Information</a>
  <ul>
  <li><a href="#error-correction-hashing-maps" id="toc-error-correction-hashing-maps" class="nav-link" data-scroll-target="#error-correction-hashing-maps"><span class="header-section-number">2.0.0.1</span> Error Correction, Hashing &amp; Maps</a></li>
  <li><a href="#cutting-edge-cross-modal-hashing" id="toc-cutting-edge-cross-modal-hashing" class="nav-link" data-scroll-target="#cutting-edge-cross-modal-hashing"><span class="header-section-number">2.1</span> Cutting edge cross-modal hashing:</a></li>
  </ul></li>
  <li><a href="#moving-bodies" id="toc-moving-bodies" class="nav-link" data-scroll-target="#moving-bodies"><span class="header-section-number">3</span> Moving Bodies</a>
  <ul>
  <li><a href="#the-mortal-computation-paradigm" id="toc-the-mortal-computation-paradigm" class="nav-link" data-scroll-target="#the-mortal-computation-paradigm"><span class="header-section-number">3.1</span> The Mortal Computation Paradigm</a></li>
  <li><a href="#theres-no-bottom-to-information-or-energy" id="toc-theres-no-bottom-to-information-or-energy" class="nav-link" data-scroll-target="#theres-no-bottom-to-information-or-energy"><span class="header-section-number">3.2</span> There’s no bottom to Information (or Energy!)</a></li>
  <li><a href="#some-predictions" id="toc-some-predictions" class="nav-link" data-scroll-target="#some-predictions"><span class="header-section-number">3.3</span> Some predictions</a>
  <ul>
  <li><a href="#potential-breakthroughs" id="toc-potential-breakthroughs" class="nav-link" data-scroll-target="#potential-breakthroughs"><span class="header-section-number">3.3.1</span> <strong>Potential Breakthroughs</strong>:</a></li>
  </ul></li>
  <li><a href="#counterpoints" id="toc-counterpoints" class="nav-link" data-scroll-target="#counterpoints"><span class="header-section-number">3.4</span> Counterpoints</a></li>
  </ul></li>
  <li><a href="#impact-on-ai-image-generation" id="toc-impact-on-ai-image-generation" class="nav-link" data-scroll-target="#impact-on-ai-image-generation"><span class="header-section-number">4</span> Impact on AI Image Generation</a></li>
  <li><a href="#moving-bodies-1" id="toc-moving-bodies-1" class="nav-link" data-scroll-target="#moving-bodies-1"><span class="header-section-number">5</span> Moving Bodies</a>
  <ul>
  <li><a href="#embodied-models-for-realistic-movement" id="toc-embodied-models-for-realistic-movement" class="nav-link" data-scroll-target="#embodied-models-for-realistic-movement"><span class="header-section-number">5.1</span> Embodied Models for Realistic Movement</a>
  <ul>
  <li><a href="#networks-and-nature" id="toc-networks-and-nature" class="nav-link" data-scroll-target="#networks-and-nature"><span class="header-section-number">5.1.1</span> Networks and Nature</a></li>
  </ul></li>
  <li><a href="#additional-considerations" id="toc-additional-considerations" class="nav-link" data-scroll-target="#additional-considerations"><span class="header-section-number">5.2</span> Additional Considerations</a></li>
  <li><a href="#wrapping-up" id="toc-wrapping-up" class="nav-link" data-scroll-target="#wrapping-up"><span class="header-section-number">5.3</span> Wrapping Up</a></li>
  <li><a href="#food-for-though-and-discussions" id="toc-food-for-though-and-discussions" class="nav-link" data-scroll-target="#food-for-though-and-discussions"><span class="header-section-number">5.4</span> Food For Though (And Discussions)</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">6</span> Conclusion</a>
  <ul>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">6.0.1</span> References</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Synthetic Intelligence &amp; Moving Bodies</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Mariana Emauz Valdetaro </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 10, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="synthetic-intelligence-moving-bodies" class="level1 unnumbered">
<h1 class="unnumbered">Synthetic Intelligence &amp; Moving Bodies</h1>
<section id="abstract" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="abstract">Abstract</h4>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>This exploratory review aims at the intersection of synthetic intelligence and embodied cognition, life and intelligence, through a panoramic and ecological lens. From theory to applications, we shall examine some recent developments in AI/ML architectures and their applications, how concepts of digital signal processing and information theory overlap into novel approaches to error correction, communication protocols and sensing capabilities in the cybernetics context, and how these developments relate to the corpus of human knowing today. We’ll do so by acknowledging that while the nature of intelligence is still elusive, recent experimental evidence seems to support a broader understanding of concepts such as “alive”, “cognizant”, and “intelligent”. Touching upon some neglected ethical considerations, such as the nature of interactions, natural accountability revisited in a diversely populated planetary system where natural-synthetic gradient of entities and systems co-exist, and lastly on the novel intelligences and the nesting of systems into multiscale competency architectures. This panoramic review of the current state and future directions of research on intelligence, AI and ML, aims to stimulate discussion and inspire further interdisciplinary collaboration. Key topics include the mortal computation thesis, cross-modal hashing techniques, error correction methods, and biomimetic approaches to AI.</p>
</div>
</div>
</div>
</section>
<section id="notes" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="notes">Notes</h3>
<p>This article is structured to provide a panoramic view of the subject, moving from broad concepts to specific applications and future implications. It begins with foundational definitions and distinctions, progresses through current challenges and innovative solutions, and culminates in a discussion of potential future developments and their societal impacts, and concluding with a brief reflection on future directions of AI research and the importance of interdisciplinary methods and teams.</p>
<p>Throughout the text, technical concepts are explained using accessible language and analogies, making the content approachable for both specialists and general readers with an interest in AI. The article series is divided into sections:</p>
<ol type="1">
<li><strong>Introduction:</strong> Here we define key terms and introduce the concept of synthetic intelligence.<br>
</li>
<li><strong>Ebbs, Flows &amp; Information:</strong> We’ll explore current challenges in AI/ML pipelines and innovative proposals.<br>
</li>
<li><strong>Moving Bodies:</strong> This section delves into the mortal computation thesis and its implications for AI.<br>
</li>
<li><strong>Additional Considerations:</strong> This part covers broader implications and related developments in the field.<br>
</li>
<li><strong>Impact on AI Image Generation:</strong> We discuss how these concepts could be applied to image and video generation.<br>
</li>
<li><strong>Conclusion:</strong> We wrap up with reflections on the future of AI and its societal impact.</li>
</ol>
<p>Each section is further divided into subsections for clarity and ease of navigation, but together the sections make up an entire piece, or booklet. A different format that also includes relevant quotes, diagrams, and code snippets to illustrate key points throughout the text.</p>
<p>By structuring the content in this way, the aim is to provide a comprehensive yet accessible overview of this pivotal techno-social-eco-logical shift. Standing at the cusp of a new era for disciplines, the internet, knowledge and humans, it seems of upmost importance to examine the convergence of various ideas - from computer science and neuroscience to philosophy and cybernetics - that are shaping the tools of today into the world and systems of tomorrow.</p>
</section>
</section>
<section id="introduction" class="level1 page-columns page-full" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<blockquote class="blockquote">
<p>“What is a friend? A single soul dwelling in two bodies.” - Aristotle</p>
</blockquote>
<section id="working-with-intelligence" class="level3 page-columns page-full" data-number="1.0.1">
<h3 data-number="1.0.1" class="anchored" data-anchor-id="working-with-intelligence"><span class="header-section-number">1.0.1</span> Working with <em>Intelligence</em>?</h3>
<p>As per convention, we first must propose a working definition of intelligence. Yet, it is truly important to accept that for now, we do not have a definition for intelligence, and most intelligent individuals will smile are the question: <strong><em>What is intelligence?</em></strong></p>
<p>Having said this, the aim is to have a functional and empirically tractable base that suits the wide-range perspective we need to talk about intelligence across domains. In practice, this means bridging the gap between theoretical concepts and practical applications in both biology and artificial intelligence, supporting the intrinsic interdisciplinary research reviewed, while continuing encouraging a more nuanced understanding of intelligence across various fields of study.</p>
<p>Given the above, we propose the following working definition of intelligence:</p>
<p><strong><em>Intelligence as the competency in navigating spaces to achieve goals / solve problems, given an environment.</em></strong></p>
<p>Here, we understand spaces as including physical, transcriptional, anatomical, physiological, and other abstract domains, combining the synthesized view from Legg and Hutter’s <span class="citation" data-cites="legg2007collection">(<a href="#ref-legg2007collection" role="doc-biblioref">Legg and Hutter 2007</a>)</span> survey of intelligence definitions with Michael Levin’s broader biological perspective <span class="citation" data-cites="levin2021technological">(<a href="#ref-levin2021technological" role="doc-biblioref">Michael Levin 2021</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-legg2007collection" class="csl-entry" role="listitem">
Legg, Shane, and Marcus Hutter. 2007. <span>“A Collection of Definitions of Intelligence.”</span> <em>arXiv Preprint arXiv:0706.3639</em>.
</div><div id="ref-levin2021technological" class="csl-entry" role="listitem">
Levin, Michael. 2021. <span>“Technological Approach to Mind Everywhere (TAME): An Experimentally-Grounded Framework for Understanding Diverse Bodies and Minds.”</span> <em>Frontiers in Systems Neuroscience</em> 15: 709301.
</div></div><p>As we explore the frontiers of synthetic intelligence and moving bodies in this article, this definition will serve as a foundational concept, guiding our discussion of AI architectures, biomimetic approaches, and the future of intelligent systems.</p>
</section>
<section id="defining-ai-and-ml-a-spectrum-of-intelligence" class="level3 page-columns page-full" data-number="1.0.2">
<h3 data-number="1.0.2" class="anchored" data-anchor-id="defining-ai-and-ml-a-spectrum-of-intelligence"><span class="header-section-number">1.0.2</span> Defining AI and ML: A Spectrum of Intelligence</h3>
<p>Artificial Intelligence (AI) and Machine Learning (ML) are often conflated, but they represent different points on the spectrum of programed intelligence. AI is a broader concept, encompassing systems designed to possess some form of intelligent means to preform a task our a set of tasks, and these for example can be rule-based, or learning-based models.</p>
<p>Machine Learning (ML), by definition expresses the ability to learn by design, and thus the field focusses on how to improve the ability to learn of a given context. Not all Artificial Intelligence systems can learn after training neither some of those based on rules that were previously inputted, so Machine Learning is a subset of AI, and focuses on systems that improve through experience, much like a child learning to recognize patterns. AI is about creating “minds”, while ML is about creating “learners”.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph TD;
    A[Artificial Intelligence: Minds] --&gt; B[Machine Learning: Learners];
    A --&gt; C[Other AI Approaches: e.g:Doers];
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>While all ML is a form of AI, not all AI is ML. Think of AI as the universe of <em>artificial</em> intelligence, <em>or should we say man made intelligence?</em>, with ML occupying a significant, but not all-encompassing, region within it.</p>
<p>This distinction is important because much like <em>Legos</em>, to build a something one must know what goes where, and windows are not very well known as foundations; a mind with a limited ability to learn is interesting on itself, but coupled with the ability to keep learning, interfacing, and experience the environment it’s then a whole different story.</p>
<p>Proposed by Joshua Bach in his 2007 thesis <strong>“Principles of Synthetic Intelligence PSI: An Architecture of Motivated Cognition”</strong> building on Dietrich Dörner’s <em>Psi theory</em>, it offered a more comprehensive and philosophically grounded approach to envision the creation of <em>true</em> intelligence, pushing the boundaries of enactive theories of agency <span class="citation" data-cites="dejesus2018">(<a href="#ref-dejesus2018" role="doc-biblioref">De Jesus 2018</a>)</span>, behaviour, purpose and teleology<span class="citation" data-cites="rosenblueth1943">(<a href="#ref-rosenblueth1943" role="doc-biblioref">Rosenblueth, Wiener, and Bigelow 1943</a>)</span>, framing that not only embodiment but composition and a variety of features would then allow for a systemic preconditions for learning and perceiving. Delving into the philosophical underpinnings of intelligence, arguing that true intelligence isn’t just about processing information, but about having goals, motivations, and the ability to navigate complex, uncertain environments.</p>
<div class="no-row-height column-margin column-container"><div id="ref-dejesus2018" class="csl-entry" role="listitem">
De Jesus, P. 2018. <span>“Thinking Through Enactive Agency: Sense-Making, Bio-Semiosis and the Ontologies of Organismic Worlds.”</span> <em>Phenomenology and Cognitive Sciences</em> 17: 861–87. <a href="https://doi.org/10.1007/s11097-018-9562-2">https://doi.org/10.1007/s11097-018-9562-2</a>.
</div><div id="ref-rosenblueth1943" class="csl-entry" role="listitem">
Rosenblueth, Arturo, Norbert Wiener, and Julian Bigelow. 1943. <span>“Behavior, Purpose and Teleology.”</span> <em>Philosophy of Science</em> 10 (1): 18–24. <a href="https://doi.org/10.1086/286788">https://doi.org/10.1086/286788</a>.
</div><div id="ref-levin2021" class="csl-entry" role="listitem">
Levin, M. 2021. <span>“Technological Approach to Mind Everywhere: An Experimentally-Grounded Framework for Understanding Diverse Bodies and Minds.”</span> <em>Frontiers in Systems Neuroscience</em> 15: 732372.
</div></div><p>This thesis has now solid and experimental evidence, combining computational and biological approaches, giving rise to a new framework, <em>technological approach to mind everywhere</em> <span class="citation" data-cites="levin2021">(<a href="#ref-levin2021" role="doc-biblioref">M. Levin 2021</a>)</span>.</p>
<p>Unlike traditional AI developments, which often focuses on isolated cognitive functions, Synthetic Intelligence <span class="citation" data-cites="bach2009">(<a href="#ref-bach2009" role="doc-biblioref">Bach 2009</a>)</span>, and the history of cybernetics as field <span class="citation" data-cites="barbrook2007">(<a href="#ref-barbrook2007" role="doc-biblioref">Barbrook 2007</a>)</span> aims to understand and replicate the holistic nature of human cognition, or it’s fundamental aspects including motivational drives that go beyond reward-base systems, emotions as systemic mediators, and enactive <span class="citation" data-cites="damiano2021">(<a href="#ref-damiano2021" role="doc-biblioref">Damiano and Stano 2021</a>)</span> behavior. Bach posits that these elements are not just add-ons to intelligence, but fundamental aspects of what it means to be intelligent.</p>
<div class="no-row-height column-margin column-container"><div id="ref-barbrook2007" class="csl-entry" role="listitem">
Barbrook, Richard. 2007. <span>“Imaginary Futures: From Thinking Machines to the Global Village,”</span> January.
</div><div id="ref-damiano2021" class="csl-entry" role="listitem">
Damiano, L., and P. Stano. 2021. <span>“A Wetware Embodied AI? Towards an Autopoietic Organizational Approach Grounded in Synthetic Biology.”</span> <em>Frontiers in Bioengineering and Biotechnology</em> 9: 724023. <a href="https://doi.org/10.3389/fbioe.2021.724023">https://doi.org/10.3389/fbioe.2021.724023</a>.
</div><div id="ref-levin2017" class="csl-entry" role="listitem">
Levin, M., G. Pezzulo, and J. M. Finkelstein. 2017. <span>“Novosphingobium Sp. PP1Y as a Model for Studying Adaptive Decisions.”</span> <em>Frontiers in Microbiology</em> 8: 2571.
</div><div id="ref-baluska2016" class="csl-entry" role="listitem">
Baluška, F., and M. Levin. 2016. <span>“On Having No Head: Cognition Throughout Biological Systems.”</span> <em>Frontiers in Psychology</em> 7: 902.
</div></div><p>Drawing parallels between the development of AI and the evolution of biological intelligence, suggesting that the challenges faced by early life forms in surviving and adapting to their environments are analogous to the challenges we face in creating truly adaptive AI <span class="citation" data-cites="levin2017">(<a href="#ref-levin2017" role="doc-biblioref">M. Levin, Pezzulo, and Finkelstein 2017</a>)</span> <span class="citation" data-cites="baluska2016">(<a href="#ref-baluska2016" role="doc-biblioref">Baluška and Levin 2016</a>)</span></p>
<!--     %%|
    label:
    fig-flow
    %%|
    fig-cap:
    "Hierarchical
    relationships
    between
    different
    types
    of
    matrices.
    *Blue
    Rectangles*
    denote
    matrices
    with
    particular,
    recognizable
    *structures*.
    *Pink
    Hexagons*
    indicate
    properties
    that
    can
    be
    *queried*."
    -->
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph TD;
    subgraph SI["Synthetic Intelligence"]
        B[Learning]
        D[Sensing]
        F[Drive]
        H[Regulation]
        C[Adapting]
        E[Input]
        G[Output]
        I[Self-Persisting]
        
        B --&gt; C
        D --&gt; E
        F --&gt; G
        H --&gt; I
    end
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>Now, here’s where it gets really interesting - this concept of Synthetic Intelligence <span class="citation" data-cites="bach2009">(<a href="#ref-bach2009" role="doc-biblioref">Bach 2009</a>)</span> lines up with Embodied Artificial Intelligence (EAI). Both of these approaches are saying, <em>“Hey, intelligence isn’t just about crunching numbers in some abstract void. It’s deeply intertwined with physical existence and how we interact with our environment.”</em></p>
<div class="no-row-height column-margin column-container"><div id="ref-bach2009" class="csl-entry" role="listitem">
Bach, J. 2009. <em>Principles of Synthetic Intelligence PSI: An Architecture of Motivated Cognition</em>. Oxford University Press.
</div><div id="ref-levin2022" class="csl-entry" role="listitem">
———. 2022. <span>“Technological Approach to Mind Everywhere (TAME): An Experimentally-Grounded Framework for Understanding Diverse Bodies and Minds.”</span> <em>Behavioral and Brain Sciences</em> 45: e58.
</div></div><p>From a developmental biology perspective, this idea is also not only valid but as mentioned above, experimental evidence suggests that there is some fundamental underpinning to intelligence, in its embodiment given the substrate where it belongs(environment) <span class="citation" data-cites="rosenblueth1943">M. Levin (<a href="#ref-levin2022" role="doc-biblioref">2022</a>)</span>.</p>
<p>Highlighting an experiemnt involving planarian flatworms <span class="citation" data-cites="shomrat2013">(<a href="#ref-shomrat2013" role="doc-biblioref">Shomrat and Levin 2013</a>)</span>, which demonstrates the relationship between embodiment, memory, and intelligence:</p>
<div class="no-row-height column-margin column-container"></div><ul>
<li><p>Planarian flatworms were trained to navigate a simple maze to find food.</p></li>
<li><p>After training, some of the worm’s heads were amputated, which in planaria leads to full regeneration of the brain.</p></li>
<li><p>Surprisingly, after regenerating their brains, the worms retained the memory of how to navigate the maze, despite having an entirely new brain <span class="citation" data-cites="shomrat2013">(<a href="#ref-shomrat2013" role="doc-biblioref">Shomrat and Levin 2013</a>)</span>.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-shomrat2013" class="csl-entry" role="listitem">
Shomrat, Tal, and Michael Levin. 2013. <span>“An Automated Training Paradigm Reveals Long-Term Memory in Planarians and Its Persistence Through Head Regeneration.”</span> <em>The Journal of Experimental Biology</em> 216 (20): 3799–3810. <a href="https://doi.org/10.1242/jeb.087809">https://doi.org/10.1242/jeb.087809</a>.
</div></div><p>This experiment suggests several key points:</p>
<ul>
<li><p>Embodied Intelligence: The memory was somehow stored in the body of the worm, not just in its brain, highlighting how intelligence and memory can be embodied in the entire organism.</p></li>
<li><p>Substrate-Independence of Mind: The fact that the memory persisted through complete brain regeneration suggests that cognitive processes might not be solely dependent on specific neural structures, but an adaptive ensemble of configurations and responses between intelligence -&gt; and environment.</p></li>
<li><p>Distributed Cognition: This experiment supports the idea that cognition and memory can be distributed throughout an organism’s body, not just centralized in a brain.</p></li>
<li><p>Bioelectric Signaling: At Levin’s Lab, their work also suggests that bioelectric signals throughout the body play a crucial role in storing and transmitting information, contributing to a form of “body-wide computation” <span class="citation" data-cites="levin2018bioelectric">(<a href="#ref-levin2018bioelectric" role="doc-biblioref">Michael Levin and Martyniuk 2018</a>)</span>.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-levin2018bioelectric" class="csl-entry" role="listitem">
Levin, Michael, and Christopher J. Martyniuk. 2018. <span>“The Bioelectric Code: An Ancient Computational Medium for Dynamic Control of Growth and Form.”</span> <em>BioSystems</em> 164: 76–93. <a href="https://doi.org/10.1016/j.biosystems.2017.08.009">https://doi.org/10.1016/j.biosystems.2017.08.009</a>.
</div></div><p>Summing up this non-exhaustive introduction to the core and relevant terms of this review, an <em>idea-wrapper</em> that seems pertinent is the <em>transdisciplinary</em> nature of this field of inquire. Right now, different areas of research seem to be finding in their experimental results insights that support ongoing research findings of different fields.</p>
<p>Ranging from philosophical debates on intelligence, sociological and economics decision-making, computers science and evolutionary and developmental sciences. This seems to suggest two different main insights:</p>
<ul>
<li><p>Intelligence beyond the <em>brain</em> &amp; AI beyond Neural Networks: Intelligence and cognition are deeply intertwined with physical embodiment and environmental interaction. These ideas also challenge traditional notions of where “mind” resides and how it operates, supporting a broader view of intelligence that does not lose specificity, but simple goes beyond what we previously considered: it being just information processing in a centralized <em>brain</em>.</p></li>
<li><p>Meta-Paradigmatic Science: Science seems to be reaching a point where a cross-disciplinary language is emerging, through field cross-pollination and hyper-specialization, where hyperpecialized and panoramic researchers leverage from each-others skills in multidisciplinary teams, tackling multi-centenary ideas that where previously deemed as having no practical application.</p></li>
</ul>
</section>
</section>
<section id="ebbs-flows-information" class="level1 page-columns page-full" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Ebbs, Flows &amp; Information</h1>
<blockquote class="blockquote">
<p>If you don’t like something, change it. If you can’t change it, change your attitude. - Maya Angelou</p>
</blockquote>
<p>We have a problem, and problems are usually present research rich opportunities.</p>
<p>Building AI / ML, had been driven by product development, which means that <em>go-to-market</em> races have been deploying architectures that may not be robust. Continuing the building construction analogy, we may think of a main ground floor of shops and markets that flourished and new economic area in town, from where many other businesses depend. Moreover, new floors are using this ground floor as a foundation. When racing for competition, and having no analogous safety and structural integrity entities ensuring those foundations, no one knows if and when an AI crumble will occur. Its also possible that it never happens, because in the mean time we develop scaffolding devices and restructuring initiatives similar to a city’s urban planning department and their site recovering in phases. Of course this paragraph is not about governmental oversight, as researchers seem more suited to advice research &amp; development teams employed by who is actively on the market, preferably recognizing the multidisciplinary importance mentioned above.</p>
<p>Current bottlenecks along AI/ML pipelines, that can be complex, extensive and costly. As an illustration of what a general sequence / elementary processes (pipeline) of an AI/ML model looks like this:</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph LR;
    A[Data Collection] --&gt; B[Preprocessing];
    B --&gt; C[Feature Extraction];
    C --&gt; D[Model Training];
    D --&gt; E[Evaluation];
    E --&gt; F[Deployment];

    style A fill:#fff,stroke:#000,stroke-width:2px,rx:10,ry:10,color:#000
    style B fill:#fff,stroke:#000,stroke-width:2px,rx:10,ry:10,color:#000
    style C fill:#fff,stroke:#000,stroke-width:2px,rx:10,ry:10,color:#000
    style D fill:#fff,stroke:#000,stroke-width:2px,rx:10,ry:10,color:#000
    style E fill:#fff,stroke:#000,stroke-width:2px,rx:10,ry:10,color:#000
    style F fill:#fff,stroke:#000,stroke-width:2px,rx:10,ry:10,color:#000
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>In the case of image generation, an easily relatable use-case scenario of AI/ML, it seems that the trend is optimizing these various stages of the process. First, an illustration of what it a general sequence / elementary processes (pipeline) of an AI/ML model:</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph LR;
    A[Input Data] --&gt; B[Noise Addition];
    B --&gt; C[Generator Network];
    C --&gt; D[Discriminator Network];
    D --&gt; E[Output Image];

    style A fill:#fff,stroke:#000,stroke-width:2px,rx:10,ry:10,color:#000
    style B fill:#fff,stroke:#000,stroke-width:2px,rx:10,ry:10,color:#000
    style C fill:#fff,stroke:#000,stroke-width:2px,rx:10,ry:10,color:#000
    style D fill:#fff,stroke:#000,stroke-width:2px,rx:10,ry:10,color:#000
    style E fill:#fff,stroke:#000,stroke-width:2px,rx:10,ry:10,color:#000
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>Image generation is a good example of cross-dependency by its multidisciplinary nature. Bottlenecks and pain-points are popular expressions that concern a general idea: Problems. And what can you do with problems? Fix them. Solving one issue versus solving the problem seems to be the main pitfall companies developing AI and tech products, because while it keeps teams busy and immediate rewards of resolving small issues, the architectural problem may not be addressed, and if they were, solving one problem could potentially resolve <em>n</em> more issues.</p>
<p>While architectural problems are of interested, they deserve an entire article (or several) to address them in more detail. For now, we may focus on one tangible questions that most (a range of technical and general audiences) can follow and leverage from the information here shared:</p>
<p><strong><em>What efforts are in place to address existing issues?</em></strong></p>
<section id="error-correction-hashing-maps" class="level4 page-columns page-full" data-number="2.0.0.1">
<h4 data-number="2.0.0.1" class="anchored" data-anchor-id="error-correction-hashing-maps"><span class="header-section-number">2.0.0.1</span> Error Correction, Hashing &amp; Maps</h4>
<p>If we perceive the flow of information and processes through the pipeline as a signal, a <em>phone call</em>, it’s easy to intuit how it is naturally prone to errors, loss, miss-calculations and interferences in the process. Most of us know what is the <a href="https://en.wikipedia.org/wiki/Telephone_game">Telephone game</a>, or the Whispers game, where <a href="https://en.wikipedia.org/wiki/Telephone_game">“which messages are whispered from person to person and then the original and final messages are compared.”</a>. Now imagine that everyone involved is dedicated to really passing the message accurately, but since they speak different languages, this will have a degree of on influence on how the message will be transmitted.</p>
<p>For decades, and actually very important in actual <em>phone-calls</em> are <em>error correction codes</em>, and they are widely used in software development as well.</p>
<p>The concepts of <strong><em>error correction codes</em>,</strong> <em>Hamming codes</em>, and <em>hashing in AI/ML</em>, have shared origins with <strong>information theory</strong>, and although a broad and historically rich research topics, and in spite their complexity, conceptually they are simple to gain some initial intuition on:</p>
<p>Imagine <strong>error correction codes</strong> as a protective shield or a self-healing fabric wrapped around data (or the <em>message</em> of the <em>Telephone Game</em> analogy above). Just as a self-healing material can repair small tears or damages, error correction codes allow data to recover from minor corruptions during transmission, or even aid the encoder fill in the gaps if data is missing. <strong>Hamming codes</strong>, in this analogy, are like a specialized patch on this fabric, designed to detect and mend <strong>specific types</strong> of tears efficiently.</p>
<p>This allows us to consider how error correction techniques play a significant role in speeding up input / output operations, feature retrieval and optimize memory usage.</p>
<p>Hashing, on the other hand, can be viewed as a form of <strong>data compression or approximation</strong>. It’s akin to creating a thumbnail of an image, <em>how much information is necessary to be passed around?</em> A smaller representation that captures the essence of the original sometimes is enough. A <em>smiley face emoji</em> does not need to be yellow if the intent is to answer <em>is it smiling? : yes or no?</em></p>
<p>In traditional computing, this thumbnail aims for uniqueness, like a fingerprint. But in AI/ML applications, it’s more like an impressionist painting, capturing the semantic essence rather than exact details.</p>
<p>The evolution of these concepts in AI/ML, particularly in <strong>cross-modal hashing and joint Hamming spaces</strong>, is like creating a universal language or a common currency for different types of information. It’s as if we’re translating diverse data types: text, images, audio; into a shared alphabet, where similarities can be easily compared regardless of their original form.</p>
<p>All these techniques, at their core, are about efficient representation and manipulation of information. They share a common ancestry in i<strong>nformation theory</strong> and the fundamental challenge of <strong>encoding, transmitting, and decoding data accurately and efficiently.</strong> Whether it’s protecting against errors, creating compact representations, or enabling cross-modal comparisons, these methods are different expressions of the same underlying principle: finding optimal ways to handle and process information in a world of imperfect channels, biases, and diverse data types.</p>
<p>In this shared universe analogy, for specialized and customized models, the range of data is somehow related. Visual culture has eras, styles etc. Thus, the Hamming distance of “A woman riding a horse on a prairie” and a “Woman sitting on a porch chair” if the model is specialized in Renascence paintings generation, is small, and their corresponding binary codes shall be small too <span class="citation" data-cites="chen2024enhancing">(<a href="#ref-chen2024enhancing" role="doc-biblioref">Chen et al. 2024b</a>)</span>. Thus, joint Hamming spaces allow for a both text and images to be correlated, and their relationship strength to be update given the model’s learning ability.</p>
<div class="no-row-height column-margin column-container"><div id="ref-chen2024enhancing" class="csl-entry" role="listitem">
——— et al. 2024b. <span>“Enhancing Cross-Modal Retrieval via Visual-Textual Prompt Hashing.”</span> In <em>Proceedings of the International Joint Conference on Artificial Intelligence</em>. Vol. 69.
</div></div><p><strong>Highlighting the most recent and used encoders:</strong><br>
- <strong>TIME</strong>: The TIME (Text-to-Image Model Editing) method can efficiently correct biases in image generator models by editing only about 1.95% of the model’s parameters (Mokady et al., 2023).<br>
- <strong>ReFACT</strong>: While ReFACT achieves even more precise results by tweaking just 0.25% of parameters while maintaining image quality (Ruiz et al., 2023).</p>
</section>
<section id="cutting-edge-cross-modal-hashing" class="level2 page-columns page-full" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="cutting-edge-cross-modal-hashing"><span class="header-section-number">2.1</span> Cutting edge cross-modal hashing:</h2>
<ul>
<li><strong>Visual-Textual Prompt Hashing (VTPH):</strong> Chen et al.&nbsp;(2024) which integrates both visual and textual prompt learning into cross-modal hashing, addressing limitations like <strong>context loss and information redundancy</strong> in existing methods. This facilitates semantic coherence between diverse modalities, improving cross-modal retrieval performance.</li>
<li><strong>Multi-Grained Similarity Preserving and Updating (MGSPU):</strong> By <span class="citation" data-cites="wu2024">(<a href="#ref-wu2024" role="doc-biblioref">Wu et al. 2024</a>)</span> this unsupervised cross-modal hashing approach combines multi-grained similarity information from local and global views to improve the accuracy of similarity measurements and preserve similarity consistency across modalities.</li>
<li><strong>Multi-Dimensional Feature Fusion Hashing (MDFFH):</strong> <span class="citation" data-cites="chen2024cross">(<a href="#ref-chen2024cross" role="doc-biblioref">Chen et al. 2024a</a>)</span> proposed the Multi-Dimensional Feature Fusion Hashing (MDFFH), where it constructs multi-dimensional fusion modules in image and text networks to learn multi-dimensional semantic features of data, integrating Vision Transformer with convolution neural networks to fuse local and global information.</li>
<li><strong>Semantic Embedding-based Online Cross-modal Hashing (SEOCH):</strong> Proposed by <span class="citation" data-cites="liu2024">(<a href="#ref-liu2024" role="doc-biblioref">Liu et al. 2024</a>)</span> SEOCH addresses the challenges of online learning for streaming data in cross-modal hashing by mapping semantic labels to a latent semantic space and employing a discrete optimization strategy for online hashing.</li>
<li><strong>Hamming Code-Based Hashing:</strong> By <span class="citation" data-cites="hinton2023">(<a href="#ref-hinton2023" role="doc-biblioref">G. Hinton 2023</a>)</span>, this approach leverages the error correction efficiency and simplicity of Hamming codes to enhance data integrity in hash-based systems.</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-wu2024" class="csl-entry" role="listitem">
Wu, Z. et al. 2024. <span>“Multi-Grained Similarity Preserving and Updating for Unsupervised Cross-Modal Hashing.”</span> <em>Applied Sciences</em> 14 (2): 870.
</div><div id="ref-chen2024cross" class="csl-entry" role="listitem">
Chen, B. et al. 2024a. <span>“Cross-Modal Retrieval Based on Multi-Dimensional Feature Fusion Hashing.”</span> <em>Frontiers in Physics</em> 12: 1379873.
</div><div id="ref-liu2024" class="csl-entry" role="listitem">
Liu, Y. et al. 2024. <span>“Semantic Embedding Based Online Cross-Modal Hashing Method.”</span> <em>Scientific Reports</em> 14 (1): 1379873.
</div><div id="ref-hinton2023" class="csl-entry" role="listitem">
Hinton, G. 2023. <span>“The Case for Digital over Analog Computation in AI.”</span>
</div></div></section>
</section>
<section id="moving-bodies" class="level1 page-columns page-full" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Moving Bodies</h1>
<blockquote class="blockquote">
<p>Life can only be understood backwards; but it must be lived forwards. - Søren Kierkegaard</p>
</blockquote>
<section id="the-mortal-computation-paradigm" class="level2 page-columns page-full" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="the-mortal-computation-paradigm"><span class="header-section-number">3.1</span> The Mortal Computation Paradigm</h2>
<p>The thesis of mortal computation is a fascinating idea that, when intersected with the future of AI/ML from both hardware and software perspectives, presents an opportunity to further narrow down the limiting ‘gap’ between those perspectives into a new realm of embodied intelligence.</p>
<p>To grasp the mortal computation thesis, we can take two roads: philosophical and physical.</p>
<p>Starting with the philosophical standpoint, death, or the finitude of life, may be seen as the the ultimate mediator for life. Without awareness of mortality, survival may be a meaningless concept, and thus could adaptative devices to survive and thrive have evolved? Of course, there are degrees of such awareness, and this is no direct claim defining what it truly means to know the essence of mortality.</p>
<p>But it does beg an intriguing question: What if we gave machines the awareness to measure and estimate computational effort, life-span of parts, and available energy sources, along with the power to control and manage some hardware/software components to improve their functioning? This concept, which some call Biomimetic Intelligence. It’s a truly transdisciplinary effort, drawing insights from biophysics, cybernetics, cognitive science, and computational biology.</p>
<p>Moving to the physical perspective, Hinton first introduced the concept of mortal computation in his lectures and talks, notably in his Romanes Lecture at the University of Oxford, and the core idea behind mortal computation is the inseparability of knowledge and hardware in computational systems. Unlike traditional digital computers where software and hardware are distinct, mortal computation proposes a model where the learned knowledge is intrinsically tied to the physical substrate it runs on. Alike the human brain, where cognitive processes are intimately linked to the neural substrate. Hinton argues that this integration could lead to more energy-efficient and potentially more powerful AI systems, albeit at the cost of the universality and perfect reproducibility offered by digital systems.</p>
<p>The theoretical underpinnings of mortal computation have been further explored and formalized in recent academic works, <span class="citation" data-cites="ororbia2023mortal">Ororbia et al. (<a href="#ref-ororbia2023mortal" role="doc-biblioref">2023</a>)</span>, synthesizes research efforts in neuroscience-inspired AI and biomimetic computing. This work frames the mortal computation thesis through the Markov blanket formalism and circular causality, underpinned by the free energy principle.</p>
<div class="no-row-height column-margin column-container"></div><p>Connecting both perspectives, the intersection of cybernetics and philosophy in this context seems very interesting. Cybernetics views living things as complex systems with interconnected parts and feedback mechanisms, rather than as simple mechanical entities. Its principles of self-regulation and homeostasis help explain how living systems maintain stability and purpose. It is intriguing how some scholars have noted parallels between cybernetic ideas and concepts from Eastern philosophies - the emphasis on interconnectedness and circular causality in cybernetics resonates with some Buddhist and Taoist concepts. Naturally, discussion on the nature of consciousness, and the implications on theoretical frameworks exploring the question,</p>
<p>This brings together several key ideas framing the mortal computation thesis through the Markov blanket formalism and circular causality <span class="citation" data-cites="hinton2022interview">Geoffrey Hinton (<a href="#ref-hinton2022interview" role="doc-biblioref">2022</a>)</span> <span class="citation" data-cites="kleiner2024consciousness">Kleiner (<a href="#ref-kleiner2024consciousness" role="doc-biblioref">2024</a>)</span> <span class="citation" data-cites="ororbia2023mortal">Ororbia et al. (<a href="#ref-ororbia2023mortal" role="doc-biblioref">2023</a>)</span>, underpinned by the free energy principle, and the nature of consciousness:</p>
<div class="no-row-height column-margin column-container"><div id="ref-hinton2022interview" class="csl-entry" role="listitem">
Hinton, Geoffrey. 2022. <span>“Geoffrey Hinton – Exclusive q&amp;a on His Latest Breakthrough.”</span> <em>Radical Ventures</em>. <a href="https://radical.vc/geoffrey-hinton-exclusive-qa-on-his-latest-breakthrough/">https://radical.vc/geoffrey-hinton-exclusive-qa-on-his-latest-breakthrough/</a>.
</div><div id="ref-kleiner2024consciousness" class="csl-entry" role="listitem">
Kleiner, Johannes. 2024. <span>“Consciousness Qua Mortal Computation.”</span> <em>arXiv Preprint arXiv:2403.03925</em>.
</div><div id="ref-ororbia2023mortal" class="csl-entry" role="listitem">
Ororbia, Alexander, Ankur Koul, Paul Fergus, Abir Hussain, and Yoonsuck Choe. 2023. <span>“Mortal Computation: A Foundation for Biomimetic Intelligence.”</span> <em>arXiv Preprint arXiv:2311.09589</em>.
</div></div><ul>
<li>It emphasizes circular causality, which is fundamental to cybernetic thinking.</li>
<li>It incorporates the concept of feedback loops, essential in both cybernetics and biomimetic systems.</li>
<li>It uses a mathematical framework to describe the boundaries of cognitive systems, relating to cybernetic ideas about system boundaries and information flow.</li>
<li>The Free Energy Principle, which underlies the mortal computation thesis, can be seen as an extension of cybernetic ideas about self-regulation and homeostasis.</li>
<li>Proposes an alternative to backpropagation in neural networks, potentially aligning more closely with biological learning processes and being more suitable for implementation in analog-like systems.</li>
</ul>
</section>
<section id="theres-no-bottom-to-information-or-energy" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="theres-no-bottom-to-information-or-energy"><span class="header-section-number">3.2</span> There’s no bottom to Information (or Energy!)</h2>
<p>One area that particularly interests me is the integration of Markov blanket formalism to better model system boundaries and information flow. This could lead to some interesting developments in how AI systems process and interpret information. The potential for more energy-efficient architectures that mimic the brain’s low-power processing is another aspect that may be promising. As we push the boundaries of AI capabilities, finding ways to reduce energy consumption will be crucial. I would risk saying that there’s no bottom to the free-energy principle in general, and in this particular case, incorporating the principle could optimize synthetic learning processes in several ways:</p>
<ol type="1">
<li>More efficient self-organization and adaptation of models to their environment</li>
<li>Improved generalization capabilities by minimizing surprise in novel situations</li>
<li>Potentially faster convergence during training due to better alignment with biological learning principles</li>
</ol>
</section>
<section id="some-predictions" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="some-predictions"><span class="header-section-number">3.3</span> Some predictions</h2>
<p>We postulate some thoughts on potential developments:</p>
<ul>
<li><p><strong>Initial Performance Drop</strong>: I suspect we might see an initial decrease in performance as we transition from current highly-optimized digital systems to more biologically-inspired models. The complexity of implementing mortal computation principles could lead to temporary setbacks in efficiency and accuracy. But this may be a necessary step towards greater advancements.</p></li>
<li><p><strong>Adaptation Period</strong>: There will likely be a period where we’ll have to adapt to new paradigms and tools. Our existing datasets and benchmarks might not be well-suited for evaluating biomimetic systems, so we’ll need to develop new evaluation methods. This could be a challenging but exciting time in the field.</p></li>
</ul>
<section id="potential-breakthroughs" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="potential-breakthroughs"><span class="header-section-number">3.3.1</span> <strong>Potential Breakthroughs</strong>:</h3>
<p>After these initial hurdles, we could see some significant leaps forward:</p>
<ul>
<li><p><strong>Efficiency Breakthrough</strong>: Once we successfully implement the principles of mortal computation, we might see dramatic improvements in energy efficiency and processing speed. This could enable much larger and more complex models to run on smaller, more affordable hardware.</p></li>
<li><p><strong>Novel Learning Capabilities</strong>: I’m particularly excited about the potential for biomimetic systems to demonstrate superior adaptability and generalization. We might see the emergence of more robust and flexible AI systems capable of handling a wider range of real-world scenarios.</p></li>
<li><p><strong>Integration with Biological Systems</strong>: The thesis opens up fascinating possibilities for better integration between AI and biological systems, envisioning advanced brain-computer interfaces or hybrid computational systems that blur the lines between artificial and biological intelligence.</p></li>
</ul>
</section>
</section>
<section id="counterpoints" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="counterpoints"><span class="header-section-number">3.4</span> Counterpoints</h2>
<p>Of course, it’s important to consider alternative viewpoints. Geoffrey Hinton, for instance, argues in favor of digital over analog (mortal) computation, citing advantages in parallelism and the implementation of backpropagation (Hinton, 2023) . While mortal computation might offer initial benefits in energy efficiency, the long-term advantages of digital systems in scalability and algorithm implementation could lead to more significant improvements.</p>
<p>I find this debate between computational paradigms fascinating, and may play an important role in shaping the future of AI hardware development.</p>
</section>
</section>
<section id="impact-on-ai-image-generation" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Impact on AI Image Generation</h1>
<p>Bringing this back to my earlier thoughts on image generation, the mortal computation thesis could potentially lead to significant improvements in current AI-ML image generation pipelines. Here are some areas where immediate development may be attainable:</p>
<ol type="1">
<li><p><strong>One-Step Generation</strong>: The recent research from Adobe and MIT on one-step diffusion models could be integrated with the mortal computation framework to drastically reduce generation time. Thus aligns perfectly with the thesis’s emphasis on efficient, biologically-inspired processing.</p></li>
<li><p><strong>Real-time Interaction</strong>: The ability to generate images 30 times faster than previous models opens up exciting possibilities for real-time, interactive image generation. This could be further enhanced by incorporating the circular causality principles from the mortal computation thesis.</p></li>
<li><p><strong>Biomimetic Neural Network Design</strong>: The thesis could guide us in developing more brain-like neural network architectures, potentially leading to more efficient and adaptable image generation models.</p></li>
<li><p><strong>Enhanced Learning and Adaptation</strong>: Incorporating the Free Energy Principle from the thesis could lead to more robust learning mechanisms in image generation models. I’m particularly excited about the potential for improved generalization and adaptation to new styles or content.</p></li>
<li><p><strong>Multimodal Integration</strong>: The thesis’s emphasis on integrating different modalities could enhance our ability to generate images from text prompts, improving the accuracy and relevance of generated content.</p></li>
</ol>
</section>
<section id="moving-bodies-1" class="level1 page-columns page-full" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Moving Bodies</h1>
<blockquote class="blockquote">
<p>I’m not interested in how people move, but what moves them. - Pina Bausch</p>
</blockquote>
<section id="embodied-models-for-realistic-movement" class="level2 page-columns page-full" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="embodied-models-for-realistic-movement"><span class="header-section-number">5.1</span> Embodied Models for Realistic Movement</h2>
<p>The potential for achieving extreme realism in moving images will likely require models to be embodied, at least at some stage of training. Recent work on motion capture and transposition, such as ByteDance’s Magic Avatar, demonstrates the ability to generate synthetic AI video with transposed actors and environments (ByteDance, 2023). Coupled with advancements in 3D environment reconstruction, like NVIDIA’s Neuralangelo could contribute to more realistic movement generation<span class="citation" data-cites="xiangli2023">(<a href="#ref-xiangli2023" role="doc-biblioref">Xiangli et al. 2023</a>)</span>, We think we may be on the cusp of some really exciting breakthroughs in generating realistic motion and interactions.</p>
<div class="no-row-height column-margin column-container"><div id="ref-xiangli2023" class="csl-entry" role="listitem">
Xiangli, Y., L. Xu, X. Pan, N. Zhao, A. Rao, C. Theobalt, B. Dai, and D. Lin. 2023. <span>“Neuralangelo: High-Fidelity Neural Surface Reconstruction.”</span>
</div></div><p>Embodiment can mean many things and have as well many interpretations. The core idea is for the software to be bound to a container, and this container be intrinsically related to the software, where the feedback mechanisms, we discussed above, loop between one adn the other passing information of internal states and the surroundings. New sensing mechanisms and protocol design must then be developed, improves and optimized.</p>
<section id="networks-and-nature" class="level3" data-number="5.1.1">
<h3 data-number="5.1.1" class="anchored" data-anchor-id="networks-and-nature"><span class="header-section-number">5.1.1</span> Networks and Nature</h3>
<p>Much is discussed on ‘Neural Networks’, and the neuron like computerized version enabling current AI/ML models. In the interest of the general public a few updates in said analogy is needed. The first is that as seem in the first section of this article, the role od bioelectric networks in evolution and intelligence is now in a maturing field of research, expand our understanding of intelence across scales. In fact, the ‘mind’ seems to be spread out the embodiment of the agent, where competencies are across these scales are then what makes up the mind, awareness, stores memory, etc. On the Multiscale Competency Architecture and TAME (Technological Approach to Mind Everywhere) frameworks, the experiments seem quite clear, for example when introducing planaria worms in a beryllium solution their brains explode. However, they regenerate their brains entirely, and more surprisingly completely adapted to the solution, now causing no harm. The suggestion is that somehow the information on the harmful chemical, and what to do to adapt to this new environment was store and processed not in the brain, but elsewhere.</p>
<p>Now, that we’ve introduce this new paradigm, a range of new possibility of architectures seems to open up for designing not only different software and models, but also hardware, or wetware for software embodiment.</p>
<p>A very well known example is the ‘Mushroom Computer’, and unlocking the mycelium’s potential for computational purposes (Adamatzky A. 2018), but the contemporary ideas of Embodied Artificial Intelligence (EAI) (Damiano, L., &amp; Stano, P. 2021) intercepted with TAME, give us the suggestion that whatever shape, form, composition and scale of the embodiment is self-informing of the kind of problems it will have the capacity to solve, and the size of it’s cognitive light cone. (ref). Roughly 30 years ago, researchers started modeling cognitive processes from the perspective of a computer and it’s architecture, and this programmatic idea stemmed from the now experimentally confirmed notion that cognition is an embedded relationship between software and hardware. Now, we may call it software and wetware as the notion transposed computer science into bioengineering and biomedicine, and from a <em>scale free cognition</em> and cybernetics perspective a cell is as much a wetware with running software as an auto-poetic system, open to evolve as the early 19th century notions of Humans being the only special kinds of agents that could learn and adapt willingly. Now we know that environments shape systems through feedback loops, within the measure that those systems may have adaptive mechanisms and their information processing throughput. Thus, the case for EAI is long established as an eminent future scenario, resting on the shoulders of previous generations of scientists, transdisciplinary ideas and ingenuity.</p>
</section>
</section>
<section id="additional-considerations" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="additional-considerations"><span class="header-section-number">5.2</span> Additional Considerations</h2>
<p>Several other factors will likely influence the future of AI and ML:</p>
<ol type="1">
<li><p><strong>AI-driven refactoring</strong>: Emerging pipelines for identifying and correcting data clumps in software repositories could significantly impact code quality and maintainability.</p></li>
<li><p><strong>Photorealism and personalization</strong>: High photorealism and personalization in avatars can lead to stronger embodiment and self-identification in virtual environments.</p></li>
<li><p><strong>Ethical considerations</strong>: As AI capabilities grow, addressing biases and ensuring ethical use becomes crucial. Methods like UCE aim to control undesirable ethical behaviors in AI models (Liu et al., 2023).</p></li>
<li><p><strong>Integration of physics principles</strong>: Models like PFGM++ demonstrate how integrating physics concepts can lead to superior performance in generative AI (Gao et al., 2023).</p></li>
<li><p><strong>Predictive maintenance and automated code review</strong>: These applications of AI in software development could significantly impact the industry.</p></li>
</ol>
</section>
<section id="wrapping-up" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="wrapping-up"><span class="header-section-number">5.3</span> Wrapping Up</h2>
<p>As we look towards the future of AI and ML, particularly in the realm of image generation and embodied intelligence, we find ourselves at the cusp of a paradigm shift. The mortal computation thesis, coupled with advancements in cross-modal hashing, error correction, and biomimetic approaches, presents exciting possibilities for more efficient, adaptable, and biologically-inspired AI systems. The integration of concepts like the Free Energy Principle and circular causality from cybernetics into AI architectures could lead to significant breakthroughs in how machines process information and interact with their environment. This could potentially result in AI systems that are not only more powerful but also more aligned with biological intelligence. However, as we navigate this new frontier, we must remain mindful of the challenges and ethical considerations that come with such advancements. The potential for extreme realism in generated content, the implications of stronger embodiment in virtual environments, and the need for responsible development of AI technologies are all critical aspects that require our attention. As we continue to push the boundaries of what’s possible in AI and ML, it’s clear that the future will be shaped by interdisciplinary approaches, combining insights from computer science, neuroscience, philosophy, and beyond. This convergence of ideas and technologies promises to unlock new realms of possibility, potentially revolutionizing our understanding of intelligence itself.</p>
</section>
<section id="food-for-though-and-discussions" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="food-for-though-and-discussions"><span class="header-section-number">5.4</span> Food For Though (And Discussions)</h2>
<p>Reflect on these developments, a key observation lays on how rapidly technology is and will likely keep evolving. There’s no shortage of fascinating areas to explore, and the scenarios we will shortly live and whiteness are completely unforeseen. Some questions that seem worth thinking about are for example “What is the potential for high photorealism and personalization in avatars to lead to stronger embodiment and self-identification in virtual environments of Humans, and how will that impact the way we relate to each other and our non-virtual environment?”, and “How will societies evolver, adapt and integrate their cultural identities, having these new domains of interaction at the peer-relations level, and new means of individual self-expression?”. Moreover, we may ask: “What kind of new necessities will rise?”, “How will this new environment shape future generations into a new stage of Human Evolution?”, and “What kind of safe guards we can design for ourselves, to ensure that what we hold as valuable now, will persist in the future?”</p>
<p>Having synthetic, intelligent, or any kind of systems we may create, bound by biology or the same physical dynamics that we Humans are, seem to me a good premise for the sustainability of all ecosystems. Moreover it could enable and ensure the relational and regulatory means necessary, given the shared meta-substrate, a closed planetary system called Earth. Simply put, and alignment of interests.</p>
<p>Additionally, a intuition tells me that when striving for literacy at all levels of society and the continuous encouragement by facilitating the access to information and knowledge, is paramount. The reverse is a then the choice of alienating some, and deliberately incapacitating democracies, thus impacting policy-making, governance and constituting an obstruction to constructive debates and participatory politics.</p>
<p>Moreover, as science and information become more complex, more specialized, a similar effort in bridging topics, simplifying language and streamlining the learning curve towards a panoramic understanding of the current knowledge map, seems, again, a sensible and tangible course of action towards current and future assurance of sense-making.</p>
</section>
</section>
<section id="conclusion" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Conclusion</h1>
<p>In conclusion, while it’s not easy to predict exactly where these developments will lead us, or if they will happen at all, neither vehemently subscribe to any idea in particular, thinking about the present and envisioning the future scope of possibilities helps us design the tomorrow we want to live in. In such, this series of opinion articles aim just that, informally and accessibly, to the best of my abilities, expose some ideas, transversely and across fields of inquiry. The future of AI and ML is not just about technological advancement, but about re-imagining the very nature of intelligence, computation, ecosystems, societies and life. As researchers, developers, and citizens, we have a responsibility to shape this future thoughtfully and responsibly.</p>
<section id="references" class="level3" data-number="6.0.1">
<h3 data-number="6.0.1" class="anchored" data-anchor-id="references"><span class="header-section-number">6.0.1</span> References</h3>



</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/mvtta\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>